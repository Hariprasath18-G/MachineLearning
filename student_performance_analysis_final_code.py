# -*- coding: utf-8 -*-
"""student performance analysis final code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RqK6As3cEaFZWG9vSdMXyqZ5rDjkuud_
"""

pip install h2o

import h2o
from h2o.estimators import H2OGradientBoostingEstimator
from h2o.frame import H2OFrame

import h2o
from h2o.estimators import H2OGradientBoostingEstimator
from h2o.frame import H2OFrame

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

df = pd.read_csv('/content/student_data.csv')

plt.figure(figsize=(8, 6))
sns.histplot(df['G1'], bins=30, kde=True,color='Red')
plt.title('Distribution of G1')
plt.xlabel('G1')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8, 6))
sns.histplot(df['G2'], bins=30, kde=True,color='Red')
plt.title('Distribution of G2')
plt.xlabel('G2')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8, 6))
sns.histplot(df['G3'], bins=30, kde=True,color='Red')
plt.title('Distribution of G3')
plt.xlabel('G3')
plt.ylabel('Frequency')
plt.show()

correlation_matrix = df.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

selected_features = ['age', 'studytime', 'failures', 'G1', 'G2', 'G3']
sns.pairplot(df[selected_features], plot_kws={'color': 'red'}, diag_kws={'color': 'red'})
plt.show()

categorical_columns = ['school', 'sex', 'address', 'Mjob', 'Fjob', 'reason']
plt.figure(figsize=(10, 8))
for i, col in enumerate(categorical_columns, 1):
    plt.subplot(2, 3, i)
    sns.countplot(x=col, data=df, color='red')  # Change color to red
    plt.title(f'Countplot of {col}')
    plt.xlabel(col)
    plt.ylabel('Count')
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
sns.boxplot(x='G3', data=df,color='red')
plt.title('Boxplot of G3')
plt.xlabel('G3')
plt.show()

plt.figure(figsize=(10, 8))
sns.violinplot(x='school', y='G3', data=df, color='red')  # Change 'palette' to 'color'
plt.title('Violin Plot of G3 by School')
plt.xlabel('School')
plt.ylabel('G3')
plt.show()

plt.figure(figsize=(12, 8))
sns.barplot(x='Mjob', y='G3', data=df,color='red')
plt.title('Average G3 by Mother\'s Job')
plt.xlabel('Mother\'s Job')
plt.ylabel('Average G3')
plt.show()

h2o.init()

predictors = ['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu',
              'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime',
              'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery',
              'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc',
              'Walc', 'health', 'absences', 'G1', 'G2']

response = 'G3'

from sklearn.model_selection import train_test_split

train, test = train_test_split(df, test_size=0.2, random_state=42)

gbm_model = H2OGradientBoostingEstimator(seed=42)

train = h2o.H2OFrame(train)
test = h2o.H2OFrame(test)

gbm_model.train(x=predictors, y=response, training_frame=train)

predictions = gbm_model.predict(test)

prob_column_names = predictions.col_names

# Find the column name containing predicted probabilities
prob_column = next((col_name for col_name in prob_column_names if 'p1' in col_name), None)

# Check if the column containing predicted probabilities is found
if prob_column:
    # Convert predicted probabilities to numpy array
    predicted_probabilities = np.array(predictions.as_data_frame()[prob_column])

    # Calculate false positive rate (FPR), true positive rate (TPR), and thresholds
    fpr, tpr, thresholds = roc_curve(actual_values, predicted_probabilities)

    # Calculate AUC
    auc = roc_auc_score(actual_values, predicted_probabilities)

    # Plot ROC curve
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.grid(True)
    plt.show()
else:
    print("Predicted probabilities column not found.")

# Inspect the structure of the predictions DataFrame
print(predictions)

# Alternatively, you can print the column names to see if the predicted probabilities are stored under a different name
print(predictions.col_names)

# from sklearn.metrics import roc_curve, auc
# from sklearn.preprocessing import label_binarize

# # Binarize the labels for multiclass classification
# binary_actual_values = label_binarize(actual_values, classes=np.unique(actual_values))

# # Initialize lists to store fpr, tpr, and auc for each class
# all_fpr = []
# all_tpr = []
# all_auc = []

# # Calculate ROC curve and AUC for each class
# for i in range(binary_actual_values.shape[1]):
#     fpr, tpr, _ = roc_curve(binary_actual_values[:, i], predicted_probabilities[:, i])
#     roc_auc = auc(fpr, tpr)
#     all_fpr.append(fpr)
#     all_tpr.append(tpr)
#     all_auc.append(roc_auc)

# # Plot ROC curve for each class
# plt.figure(figsize=(8, 6))
# for i in range(binary_actual_values.shape[1]):
#     plt.plot(all_fpr[i], all_tpr[i], label=f'Class {i} (AUC = {all_auc[i]:.2f})')

# plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
# plt.xlabel('False Positive Rate')
# plt.ylabel('True Positive Rate')
# plt.title('ROC Curve (One-vs-Rest)')
# plt.legend()
# plt.grid(True)
# plt.show()

print(gbm_model)

mse = gbm_model.mse()
rmse = gbm_model.rmse()
mae = gbm_model.mae()

print(f"MSE: {mse}")
print(f"RMSE: {rmse}")
print(f"MAE: {mae}")

predictions.head()

import matplotlib.pyplot as plt

# Convert H2OFrame to pandas DataFrame for visualization
test_pd = test.as_data_frame()

# Get the actual values from the test set
actual_values = test_pd[response].values

# Get the predicted values from the H2O model
predicted_values = predictions.as_data_frame()['predict'].values

# Define the color you want for the points
point_color = 'red'  # You can specify a color name or hexadecimal code

# Create a scatter plot with the specified color
plt.figure(figsize=(10, 6))
plt.scatter(actual_values, predicted_values, alpha=0.5, color=point_color)
plt.title('Actual vs Predicted G3 Scores (H2O Gradient Boosting)')
plt.xlabel('Actual G3 Scores')
plt.ylabel('Predicted G3 Scores')
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Convert H2OFrame to pandas DataFrame for visualization
test_pd = test.as_data_frame()

# Get the actual values from the test set
actual_values = test_pd[response].values

# Get the predicted values from the H2O model
predicted_values = predictions.as_data_frame()['predict'].values

# Define a color map
color_map = plt.cm.viridis

# Normalize values for the color map
norm = plt.Normalize(actual_values.min(), actual_values.max())
colors = color_map(norm(actual_values))

# Create a scatter plot with different colors
plt.figure(figsize=(10, 6))
plt.scatter(actual_values, predicted_values, alpha=0.5, color=colors)
plt.title('Actual vs Predicted G3 Scores (H2O Gradient Boosting)')
plt.xlabel('Actual G3 Scores')
plt.ylabel('Predicted G3 Scores')
plt.colorbar(label='Actual G3 Scores')
plt.show()

df['G1'].hist(bins=20, edgecolor='black', alpha=0.7,color='red')
plt.title('Histogram of G1 Scores')
plt.xlabel('G1 Scores')
plt.ylabel('Frequency')
plt.show()

df['G2'].hist(bins=20, edgecolor='black', alpha=0.7)
plt.title('Histogram of G2 Scores')
plt.xlabel('G2 Scores')
plt.ylabel('Frequency')
plt.show()

mae_value = 0.38358993959153376  # Replace with your actual MAE value
mse_value = 0.3249036571086849  # Replace with your actual MSE value

# Create a bar plot for MAE and MSE with reduced width
plt.figure(figsize=(6, 6))  # Adjust figsize to reduce width
plt.bar(['MAE', 'MSE'], [mae_value, mse_value], color=['blue', 'orange'], width=0.5)  # Adjust width here
plt.title('Model Evaluation Metrics')
plt.ylabel('Metric Value')
plt.show()

# Importing necessary libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load the dataset (replace 'dataset.csv' with the actual dataset file)
data = pd.read_csv('/content/student_data.csv')

# Assuming 'G3' as the target variable and other columns as features
target_variable = 'G3'             # Target variable
features = data.drop(target_variable, axis=1)  # Features
target = data[target_variable]     # Target variable

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# One-hot encode categorical variables
features = pd.get_dummies(features)

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Initializing the Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Training the classifier
rf_classifier.fit(X_train, y_train)

# Making predictions on the test set
y_pred = rf_classifier.predict(X_test)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Printing classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

from sklearn.metrics import mean_squared_error, mean_squared_log_error

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error (MSE):", mse)

# Calculate Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)
print("Root Mean Squared Error (RMSE):", rmse)

# Calculate Root Mean Log Squared Error (RMLSE)
y_pred_log = np.log1p(y_pred)  # Log transform predictions
y_test_log = np.log1p(y_test)  # Log transform true values
rmlse = np.sqrt(mean_squared_log_error(y_test_log, y_pred_log))
print("Root Mean Log Squared Error (RMLSE):", rmlse)

from sklearn.metrics import mean_absolute_error

# Calculate Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error (MAE):", mae)

# Calculate residuals
residuals = y_test - y_pred

# Calculate Mean Residual Deviance
mean_residual_deviance = np.mean(residuals ** 2)
print("Mean Residual Deviance:", mean_residual_deviance)

import numpy as np
import matplotlib.pyplot as plt

# Metrics and their values
metrics = ['MSE', 'RMSE', 'MAE', 'RMSLE', 'Mean Residual Deviance']
values = [0.3249036571086849, 0.5700032079810472, 0.38358993959153376, 0.20066656476412018, 0.3249036571086849]

# Number of metrics
num_metrics = len(metrics)

# Compute angle for each axis
angles = np.linspace(0, 2 * np.pi, num_metrics, endpoint=False).tolist()

# Close the plot
values += values[:1]
angles += angles[:1]

# Plot
fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))
ax.fill(angles, values, color='blue', alpha=0.25)
ax.plot(angles, values, color='blue', linewidth=2)
ax.set_yticklabels([])
ax.set_xticks(angles[:-1])
ax.set_xticklabels(metrics)
ax.set_title('Model Evaluation Metrics', size=20, color='blue', pad=20)

plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Metrics and their values
metrics = ['MSE', 'RMSE', 'MAE', 'RMSLE', 'Mean Residual Deviance']
values = [0.3249036571086849, 0.5700032079810472, 0.38358993959153376, 0.20066656476412018, 0.3249036571086849]

# Number of metrics
num_metrics = len(metrics)

# Compute angle for each axis
angles = np.linspace(0, 2 * np.pi, num_metrics, endpoint=False).tolist()

# Close the plot
values += values[:1]
angles += angles[:1]

# Plot
fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))
ax.plot(angles, values, color='blue', linewidth=2, linestyle='solid', marker='o', markersize=8, label='Metrics')
ax.fill(angles, values, color='blue', alpha=0.25)
ax.set_yticklabels([])
ax.set_xticks(angles[:-1])
ax.set_xticklabels(metrics)
ax.set_title('Model Evaluation Metrics', size=20, color='blue', pad=20)
ax.legend(loc='upper right')

plt.show()

import matplotlib.pyplot as plt

# Metrics and their values
metrics = ['MSE', 'RMSE', 'MAE', 'RMSLE', 'Mean Residual Deviance']
values = [0.3249036571086849, 0.5700032079810472, 0.38358993959153376, 0.20066656476412018, 0.3249036571086849]

# Create spike plot
plt.figure(figsize=(10, 6))
plt.stem(metrics, values, linefmt='red', markerfmt='bo', basefmt=' ', use_line_collection=True)
plt.title('Model Evaluation Metrics')
plt.xlabel('Metrics')
plt.ylabel('Values')
plt.xticks(rotation=45)
plt.grid(axis='y')

# Show plot
plt.show()

import matplotlib.pyplot as plt

# Metrics and their values
metrics = ['MSE', 'RMSE', 'MAE', 'RMSLE', 'Mean Residual Deviance']
values = [0.3249036571086849, 0.5700032079810472, 0.38358993959153376, 0.20066656476412018, 0.3249036571086849]

# Create spike plot with sharp spikes
plt.figure(figsize=(10, 6))
plt.stem(metrics, values, linefmt='blue', markerfmt='b^', basefmt=' ', use_line_collection=True)
plt.title('Model Evaluation Metrics')
plt.xlabel('Metrics')
plt.ylabel('Values')
plt.xticks(rotation=45)
plt.grid(axis='y')

# Show plot
plt.show()

import matplotlib.pyplot as plt

# Metrics and their values
metrics = ['MSE', 'RMSE', 'MAE', 'RMSLE']
values = [0.3249036571086849, 0.5700032079810472, 0.38358993959153376, 0.20066656476412018]

# Create side-by-side bar plot
plt.figure(figsize=(6, 6))
plt.bar(metrics, values, color=['skyblue', 'green', 'orange', 'red'])
plt.xlabel('Metrics')
plt.ylabel('Values')
plt.title('Model Evaluation Metrics Comparison')
plt.xticks(rotation=45)
plt.tight_layout()

# Show plot
plt.show()

# Get feature importances from the H2O model
feature_importances = gbm_model.varimp(use_pandas=True)

# Sort feature importances by relative importance
feature_importances.sort_values(by='relative_importance', ascending=False, inplace=True)

# Create a bar plot of feature importances
plt.figure(figsize=(12, 8))
sns.barplot(x='relative_importance', y='variable', data=feature_importances, palette='viridis')
plt.title('Feature Importance Plot (H2O Gradient Boosting)')
plt.xlabel('Relative Importance')
plt.ylabel('Feature')
plt.show()



import matplotlib.pyplot as plt

# Metrics for the random classifier
random_classifier_metrics = {
    "Mean Squared Error (MSE)": 9.544303797468354,
    "Root Mean Squared Error (RMSE)": 3.089385666676848,
    "Root Mean Log Squared Error (RMLSE)": 0.35780440422812676
}

# Metrics for H2O
h2o_metrics = {
    "Mean Squared Error (MSE)": 0.3249036571086849,
    "Root Mean Squared Error (RMSE)": 0.5700032079810472,
    "Root Mean Log Squared Error (RMLSE)": 0.38358993959153376,
    "Mean Absolute Error (MAE)": 0.20066656476412018
}

# Extracting metric names and values
metric_names = list(random_classifier_metrics.keys())
random_classifier_values = list(random_classifier_metrics.values())
h2o_values = [h2o_metrics[name] if name in h2o_metrics else None for name in metric_names]

# Plotting the comparison
plt.figure(figsize=(10, 6))
bar_width = 0.35
index = range(len(metric_names))

plt.bar(index, random_classifier_values, bar_width, label='Random Classifier')
plt.bar(index, h2o_values, bar_width, label='H2O', alpha=0.5)

plt.xlabel('Metrics')
plt.ylabel('Values')
plt.title('Comparison of Metrics: Random Classifier vs. H2O')
plt.xticks(index, metric_names, rotation=45)
plt.legend()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Metrics for the random classifier
random_classifier_metrics = {
    "MSE": 9.544303797468354,
    "RMSE": 3.089385666676848,
    "RMLSE": 0.35780440422812676,
    "Mean Residual Deviance": 9.544303797468354
}

# Metrics for H2O
h2o_metrics = {
    "MSE": 0.3249036571086849,
    "RMSE": 0.5700032079810472,
    "MAE": 0.38358993959153376,
    "RMSLE": 0.20066656476412018,
    "Mean Residual Deviance": 0.3249036571086849
}

# Find common metrics
common_metrics = set(random_classifier_metrics.keys()).intersection(h2o_metrics.keys())

# Extract common metric names and values
metric_names = list(common_metrics)
random_classifier_values = [random_classifier_metrics[name] for name in metric_names]
h2o_values = [h2o_metrics[name] for name in metric_names]

# Plotting the comparison
plt.figure(figsize=(10, 6))

plt.bar(metric_names, random_classifier_values, label='Random Classifier', color='b', alpha=0.7)
plt.bar(metric_names, h2o_values, label='H2O', color='r', alpha=0.7)

plt.xlabel('Metrics')
plt.ylabel('Values')
plt.title('Comparison of Metrics: Random Classifier vs. H2O')
plt.xticks(rotation=45)
plt.legend()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Metrics for the random classifier
random_classifier_metrics = {
    "MSE": 9.544303797468354,
    "RMSE": 3.089385666676848,
    "RMLSE": 0.35780440422812676,
    "Mean Residual Deviance": 9.544303797468354
}

# Metrics for H2O
h2o_metrics = {
    "MSE": 0.3249036571086849,
    "RMSE": 0.5700032079810472,
    "MAE": 0.38358993959153376,
    "RMSLE": 0.20066656476412018,
    "Mean Residual Deviance": 0.3249036571086849
}

# Common metrics
common_metrics = set(random_classifier_metrics.keys()).intersection(h2o_metrics.keys())

# Extract common metric names and values
metric_names = list(common_metrics)
random_classifier_values = [random_classifier_metrics[name] for name in metric_names]
h2o_values = [h2o_metrics[name] for name in metric_names]

# Plotting the comparison using a line plot
plt.figure(figsize=(10, 6))

for model, values in [("Random Classifier", random_classifier_values), ("H2O", h2o_values)]:
    plt.plot(metric_names, values, marker='o', label=model)

plt.xlabel('Metrics')
plt.ylabel('Values')
plt.title('Comparison of Metrics: Random Classifier vs. H2O')
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

accuracy = correct_predictions / total_predictions

print("Accuracy:", accuracy)

import h2o
from h2o.estimators import H2OGradientBoostingEstimator
from sklearn.model_selection import train_test_split

# Initialize and start the H2O cluster
h2o.init()

# Load the dataset
df = pd.read_csv('/content/student_data.csv')

# Define predictors and response variable
predictors = ['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu',
              'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime',
              'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery',
              'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc',
              'Walc', 'health', 'absences', 'G1', 'G2']
response = 'G3'

# Split data into train and test sets
train, test = train_test_split(df, test_size=0.2, random_state=42)

# Convert train and test sets to H2OFrame
train_h2o = h2o.H2OFrame(train)
test_h2o = h2o.H2OFrame(test)

# Train the H2O Gradient Boosting model
gbm_model = H2OGradientBoostingEstimator(seed=42)
gbm_model.train(x=predictors, y=response, training_frame=train_h2o)

# Make predictions on the test data
predictions = gbm_model.predict(test_h2o)

# Get actual and predicted values
actual_values = test_h2o[response].as_data_frame()['G3'].values
predicted_values = predictions.as_data_frame()['predict'].values

# Calculate accuracy
accuracy = sum(actual_values == predicted_values) / len(actual_values)
print(f"Accuracy: {accuracy}")

# Shutdown H2O cluster
h2o.shutdown()

